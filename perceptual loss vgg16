{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":2008381,"sourceType":"datasetVersion","datasetId":1201791}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Cell 1: Imports and Setup\nimport os\nimport glob\nimport numpy as np\nfrom PIL import Image\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nfrom skimage.color import rgb2lab, lab2rgb\nfrom skimage import io\nimport torchvision\nfrom torchvision import transforms\nfrom torchvision.models import vgg16, VGG16_Weights                #\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.nn.utils import spectral_norm\n\nimport torch\nfrom torch import nn, optim\n\n# Device configuration\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-07T16:11:20.442044Z","iopub.execute_input":"2025-04-07T16:11:20.442371Z","iopub.status.idle":"2025-04-07T16:11:20.448991Z","shell.execute_reply.started":"2025-04-07T16:11:20.442346Z","shell.execute_reply":"2025-04-07T16:11:20.447979Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 2: Data Loading and Preprocessing\n\n# Define the base directory of your dataset\nbase_dir = \"/kaggle/input/sentinel12-image-pairs-segregated-by-terrain/v_2\"\ncategories = ['agri', 'barrenland', 'grassland', 'urban']\n\n# Collect all pairs of input (SAR) and output (optical) image paths\ninput_output_pairs = []\nfor category in categories:\n    input_folder = os.path.join(base_dir, category, 's1')\n    output_folder = os.path.join(base_dir, category, 's2')\n    \n    # Get all input and output image file paths\n    input_images = sorted(glob.glob(os.path.join(input_folder, \"*.png\")))\n    output_images = sorted(glob.glob(os.path.join(output_folder, \"*.png\")))\n    \n    # Ensure that the number of images match\n    assert len(input_images) == len(output_images), \\\n        f\"Number of images in {input_folder} and {output_folder} do not match.\"\n    \n    for input_img, output_img in zip(input_images, output_images):\n        input_output_pairs.append((input_img, output_img))\n\n# Checking the size of the dataset\nprint(f\"Total dataset size: {len(input_output_pairs)}\")\n\n# Shuffle and split the dataset into training and validation sets\nnp.random.seed(123)  # Seeding for reproducibility\ninput_output_pairs = np.random.permutation(input_output_pairs)  # Shuffling the pairs\n\n# Splitting into training and validation sets\ntrain_ratio = 0.8\nnum_total = len(input_output_pairs)\nnum_train = int(train_ratio * num_total)\n\ntrain_pairs = input_output_pairs[:num_train]\nval_pairs = input_output_pairs[num_train:]\n\nprint(f\"Training set size: {len(train_pairs)}\")\nprint(f\"Validation set size: {len(val_pairs)}\")\n\n# Example: Accessing a pair\nexample_input, example_output = train_pairs[0]\nprint(\"Example input image path:\", example_input)\nprint(\"Example output image path:\", example_output)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T16:11:20.450197Z","iopub.execute_input":"2025-04-07T16:11:20.450422Z","iopub.status.idle":"2025-04-07T16:11:20.581258Z","shell.execute_reply.started":"2025-04-07T16:11:20.450396Z","shell.execute_reply":"2025-04-07T16:11:20.580407Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 3: Utility Functions\n\n# AverageMeter for tracking losses\nclass AverageMeter:\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.count, self.avg, self.sum = [0.] * 3\n\n    def update(self, val, count=1):\n        self.count += count\n        self.sum += count * val\n        self.avg = self.sum / self.count\n\n# Function to convert Lab to RGB\ndef lab_to_rgb(L, ab):\n    L = (L + 1.) * 50.  # Denormalize L channel from [-1, 1] to [0, 100]\n    ab = ab * 110.  # Denormalize ab channels from [-1, 1] to [-110, 110]\n    Lab = torch.cat([L, ab], dim=1).permute(0, 2, 3, 1).cpu().detach().numpy()  # Shape: [batch_size, H, W, 3]\n    rgb_imgs = []\n    for img in Lab:\n        img_rgb = lab2rgb(img.astype('float64'))\n        rgb_imgs.append(img_rgb)\n    return np.stack(rgb_imgs, axis=0)\n\n# Visualization function\ndef visualize(model, data, save=False, epoch=0):\n    model.net_G.eval()\n    with torch.no_grad():\n        model.setup_input(data)\n        model.forward()\n    fake_color = model.fake_color.detach()\n    real_color = model.ab\n    L = model.L\n    fake_imgs = lab_to_rgb(L, fake_color)\n    real_imgs = lab_to_rgb(L, real_color)\n    fig = plt.figure(figsize=(15, 8))\n    for i in range(5):\n        ax = plt.subplot(3, 5, i + 1)\n        ax.imshow(L[i][0].cpu(), cmap='gray')\n        ax.axis(\"off\")\n        if i == 0:\n            ax.set_title('Input L (SAR)')\n        ax = plt.subplot(3, 5, i + 1 + 5)\n        ax.imshow(fake_imgs[i])\n        ax.axis(\"off\")\n        if i == 0:\n            ax.set_title('Generated RGB')\n        ax = plt.subplot(3, 5, i + 1 + 10)\n        ax.imshow(real_imgs[i])\n        ax.axis(\"off\")\n        if i == 0:\n            ax.set_title('Ground Truth RGB')\n    plt.tight_layout()\n    if save:\n        plt.savefig(f\"colorization_epoch_{epoch}.png\")\n    plt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T16:11:20.582986Z","iopub.execute_input":"2025-04-07T16:11:20.583237Z","iopub.status.idle":"2025-04-07T16:11:20.591725Z","shell.execute_reply.started":"2025-04-07T16:11:20.583217Z","shell.execute_reply":"2025-04-07T16:11:20.590961Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 4: Custom Dataset Class\n\nclass ColorizationDataset(Dataset):\n    def __init__(self, pairs, split='train', transform=None):\n        self.pairs = pairs\n        self.split = split\n        self.size = 256  # Image size\n        self.transform = transform\n\n        # Define default transforms if none are provided\n        if self.transform is None:\n            if self.split == 'train':\n                self.transform = transforms.Compose([\n                    transforms.Resize((self.size, self.size), Image.BICUBIC),\n                    transforms.RandomHorizontalFlip(),\n                ])\n            else:\n                self.transform = transforms.Resize((self.size, self.size), Image.BICUBIC)\n    \n    def __len__(self):\n        return len(self.pairs)\n    \n    def __getitem__(self, idx):\n        input_path, output_path = self.pairs[idx]\n        \n        # Load the input (SAR) image and convert to grayscale\n        input_img = Image.open(input_path).convert('L')  # Ensure it's grayscale\n        input_img = self.transform(input_img)\n        input_img = transforms.ToTensor()(input_img)  # Shape: [1, H, W]\n        \n        # Load the output (optical) image and convert to RGB\n        output_img = Image.open(output_path).convert('RGB')\n        output_img = self.transform(output_img)\n        output_img = transforms.ToTensor()(output_img)  # Shape: [3, H, W]\n        \n        # Convert output image from RGB to Lab color space\n        output_img_np = output_img.permute(1, 2, 0).numpy()  # Convert to HWC\n        output_lab = rgb2lab(output_img_np).astype('float32')\n        output_lab = transforms.ToTensor()(output_lab)  # Shape: [3, H, W]\n        \n        # Normalize L and ab channels\n        L = input_img * 2.0 - 1.0  # Normalize L channel to [-1, 1]\n        ab = output_lab[1:, ...] / 110.  # Normalize ab channels to [-1, 1]\n        \n        return {'L': L, 'ab': ab}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T16:11:20.592741Z","iopub.execute_input":"2025-04-07T16:11:20.593044Z","iopub.status.idle":"2025-04-07T16:11:20.610509Z","shell.execute_reply.started":"2025-04-07T16:11:20.593016Z","shell.execute_reply":"2025-04-07T16:11:20.609727Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 5: Data Loaders\n\ndef make_dataloaders(pairs, batch_size=8, num_workers=4, split='train'):\n    dataset = ColorizationDataset(pairs, split=split)\n    dataloader = DataLoader(\n        dataset,\n        batch_size=batch_size,\n        shuffle=(split=='train'),\n        num_workers=num_workers,\n        pin_memory=True\n    )\n    return dataloader\n\n# Create data loaders\nbatch_size = 8  # Reduced batch size to fit in memory\ntrain_dl = make_dataloaders(train_pairs, batch_size=batch_size, num_workers=4, split='train')\nval_dl = make_dataloaders(val_pairs, batch_size=batch_size, num_workers=4, split='val')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T16:11:20.611242Z","iopub.execute_input":"2025-04-07T16:11:20.611526Z","iopub.status.idle":"2025-04-07T16:11:20.627686Z","shell.execute_reply.started":"2025-04-07T16:11:20.611499Z","shell.execute_reply":"2025-04-07T16:11:20.627024Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 6: UnetBlock without Self-Attention\n\nclass UnetBlock(nn.Module):\n    def __init__(self, nf, ni, submodule=None, input_c=None, dropout=False,\n                 innermost=False, outermost=False):\n        super().__init__()\n        self.outermost = outermost\n        if input_c is None:\n            input_c = nf\n        downconv = nn.Conv2d(input_c, ni, kernel_size=4, stride=2,\n                             padding=1, bias=False)\n        downrelu = nn.LeakyReLU(0.2, True)\n        downnorm = nn.BatchNorm2d(ni)\n        uprelu = nn.ReLU(True)\n        upnorm = nn.BatchNorm2d(nf)\n\n        if outermost:\n            upconv = nn.ConvTranspose2d(ni * 2, nf, kernel_size=4, stride=2,\n                                        padding=1)\n            down = [downconv]\n            up = [uprelu, upconv, nn.Tanh()]\n            model = down + [submodule] + up\n        elif innermost:\n            upconv = nn.ConvTranspose2d(ni, nf, kernel_size=4, stride=2,\n                                        padding=1, bias=False)\n            down = [downrelu, downconv]\n            up = [uprelu, upconv, upnorm]\n            model = down + up\n        else:\n            upconv = nn.ConvTranspose2d(ni * 2, nf, kernel_size=4, stride=2,\n                                        padding=1, bias=False)\n            down = [downrelu, downconv, downnorm]\n            up = [uprelu, upconv, upnorm]\n            if dropout:\n                up += [nn.Dropout(0.5)]\n            model = down + [submodule] + up\n\n        self.model = nn.Sequential(*model)\n\n    def forward(self, x):\n        if self.outermost:\n            return self.model(x)\n        else:\n            return torch.cat([x, self.model(x)], 1)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T16:11:20.631447Z","iopub.execute_input":"2025-04-07T16:11:20.631711Z","iopub.status.idle":"2025-04-07T16:11:20.647292Z","shell.execute_reply.started":"2025-04-07T16:11:20.631681Z","shell.execute_reply":"2025-04-07T16:11:20.646512Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 7: U-Net Generator\n\nclass UnetGenerator(nn.Module):\n    def __init__(self, input_c=1, output_c=2, num_downs=8, num_filters=64):\n        super().__init__()\n        unet_block = UnetBlock(num_filters * 8, num_filters * 8, innermost=True)\n        for _ in range(num_downs - 5):\n            unet_block = UnetBlock(num_filters * 8, num_filters * 8,\n                                   submodule=unet_block, dropout=True)\n        unet_block = UnetBlock(num_filters * 4, num_filters * 8,\n                               submodule=unet_block)\n        unet_block = UnetBlock(num_filters * 2, num_filters * 4,\n                               submodule=unet_block)\n        unet_block = UnetBlock(num_filters, num_filters * 2,\n                               submodule=unet_block)\n        self.model = UnetBlock(output_c, num_filters, input_c=input_c,\n                               submodule=unet_block, outermost=True)\n\n    def forward(self, x):\n        return self.model(x)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T16:11:20.648201Z","iopub.execute_input":"2025-04-07T16:11:20.648447Z","iopub.status.idle":"2025-04-07T16:11:20.664360Z","shell.execute_reply.started":"2025-04-07T16:11:20.648427Z","shell.execute_reply":"2025-04-07T16:11:20.663582Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 8: Discriminator with Spectral Normalization\n\nclass PatchDiscriminatorSN(nn.Module):\n    def __init__(self, input_c, num_filters=64, n_down=3):\n        super().__init__()\n        layers = [self.get_layers(input_c, num_filters, norm=False)]\n        nf_mult = 1\n        for n in range(1, n_down):\n            nf_mult_prev = nf_mult\n            nf_mult = min(2 ** n, 8)\n            layers += [self.get_layers(num_filters * nf_mult_prev,\n                                       num_filters * nf_mult, s=2)]\n        nf_mult_prev = nf_mult\n        nf_mult = min(2 ** n_down, 8)\n        layers += [self.get_layers(num_filters * nf_mult_prev,\n                                   num_filters * nf_mult, s=1)]\n        layers += [spectral_norm(nn.Conv2d(num_filters * nf_mult, 1,\n                                           kernel_size=4, stride=1, padding=1))]\n        self.model = nn.Sequential(*layers)\n\n    def get_layers(self, in_c, out_c, k=4, s=2, p=1, norm=True):\n        layers = [spectral_norm(nn.Conv2d(in_c, out_c, kernel_size=k,\n                                          stride=s, padding=p))]\n        if norm:\n            layers.append(nn.BatchNorm2d(out_c))\n        layers.append(nn.LeakyReLU(0.2, inplace=True))\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        return self.model(x)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T16:11:20.666041Z","iopub.execute_input":"2025-04-07T16:11:20.666232Z","iopub.status.idle":"2025-04-07T16:11:20.682817Z","shell.execute_reply.started":"2025-04-07T16:11:20.666215Z","shell.execute_reply":"2025-04-07T16:11:20.681963Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 9: GAN Loss\n\nclass GANLoss(nn.Module):\n    def __init__(self, gan_mode='vanilla', real_label=1.0, fake_label=0.0):\n        super().__init__()\n        self.register_buffer('real_label', torch.tensor(real_label))\n        self.register_buffer('fake_label', torch.tensor(fake_label))\n        if gan_mode == 'vanilla':\n            self.loss = nn.BCEWithLogitsLoss()\n        elif gan_mode == 'lsgan':\n            self.loss = nn.MSELoss()\n\n    def get_labels(self, preds, target_is_real):\n        labels = self.real_label if target_is_real else self.fake_label\n        return labels.expand_as(preds)\n\n    def forward(self, preds, target_is_real):\n        labels = self.get_labels(preds, target_is_real)\n        loss = self.loss(preds, labels)\n        return loss\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T16:11:20.684037Z","iopub.execute_input":"2025-04-07T16:11:20.684244Z","iopub.status.idle":"2025-04-07T16:11:20.703457Z","shell.execute_reply.started":"2025-04-07T16:11:20.684225Z","shell.execute_reply":"2025-04-07T16:11:20.702672Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom torchvision import models\n\n# Load VGG19 and inspect the architecture\nvgg16_model = models.vgg16(weights=models.VGG16_Weights.DEFAULT)\n\n# Print out the model to see the layers\nprint(vgg16_model.features)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T16:11:20.704387Z","iopub.execute_input":"2025-04-07T16:11:20.704659Z","iopub.status.idle":"2025-04-07T16:11:22.341782Z","shell.execute_reply.started":"2025-04-07T16:11:20.704631Z","shell.execute_reply":"2025-04-07T16:11:22.340916Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 10: Perceptual Loss\n\nclass PerceptualLoss(nn.Module):\n    def __init__(self, feature_layers=[0, 5, 10, 19, 28], weights=[1.0]*5):\n    # def __init__(self, feature_layers=[0, 5, 10, 14, 19], weights=[1.0]*5):\n        super(PerceptualLoss, self).__init__()\n        vgg_weights = VGG16_Weights.DEFAULT\n        self.vgg = vgg16(weights=vgg_weights).features[:max(feature_layers)+1].to(device).eval()\n        for param in self.vgg.parameters():\n            param.requires_grad = False\n        self.feature_layers = feature_layers\n        self.weights = weights\n\n    def forward(self, pred, target):\n        # Since the predicted and target images are ab channels, we need to create 3-channel images\n        # We'll concatenate the L channel with the ab channels to form Lab images, then convert to RGB\n        # For perceptual loss, we need RGB images with 3 channels\n\n        # Reconstruct Lab images\n        L = torch.zeros_like(pred[:, :1, :, :]).to(device)  # Dummy L channel\n        pred_lab = torch.cat([L, pred], dim=1)\n        target_lab = torch.cat([L, target], dim=1)\n\n        # Convert Lab to RGB\n        pred_rgb = lab_to_rgb(L, pred)\n        target_rgb = lab_to_rgb(L, target)\n\n        # Convert to tensors\n        pred_rgb = torch.from_numpy(pred_rgb).permute(0, 3, 1, 2).to(device).float()\n        target_rgb = torch.from_numpy(target_rgb).permute(0, 3, 1, 2).to(device).float()\n\n        # Normalize RGB images to [-1, 1]\n        pred_rgb = (pred_rgb / 0.5) - 1.0\n        target_rgb = (target_rgb / 0.5) - 1.0\n\n        loss = 0.0\n        x = pred_rgb\n        y = target_rgb\n        for i, layer in enumerate(self.vgg):\n            x = layer(x)\n            y = layer(y)\n            if i in self.feature_layers:\n                loss += self.weights[self.feature_layers.index(i)] * nn.functional.l1_loss(x, y)\n        return loss\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T16:11:22.342675Z","iopub.execute_input":"2025-04-07T16:11:22.342941Z","iopub.status.idle":"2025-04-07T16:11:22.350191Z","shell.execute_reply.started":"2025-04-07T16:11:22.342910Z","shell.execute_reply":"2025-04-07T16:11:22.349418Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 11: Model Initialization\n\ndef init_weights(net, init='kaiming'):\n    def init_func(m):\n        classname = m.__class__.__name__\n        if hasattr(m, 'weight') and ('Conv' in classname or 'Linear' in classname):\n            if init == 'kaiming':\n                nn.init.kaiming_normal_(m.weight.data, a=0, mode='fan_in')\n            elif init == 'normal':\n                nn.init.normal_(m.weight.data, mean=0.0, std=0.02)\n            if hasattr(m, 'bias') and m.bias is not None:\n                nn.init.constant_(m.bias.data, 0.0)\n    net.apply(init_func)\n    return net\n\ndef init_model(model):\n    model = model.to(device)\n    model = init_weights(model)\n    return model\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T16:11:22.350910Z","iopub.execute_input":"2025-04-07T16:11:22.351127Z","iopub.status.idle":"2025-04-07T16:11:22.368801Z","shell.execute_reply.started":"2025-04-07T16:11:22.351109Z","shell.execute_reply":"2025-04-07T16:11:22.368062Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 12: MainModel Class\n\nclass MainModel(nn.Module):\n    def __init__(self, net_G=None, net_D=None, lr_G=2e-4, lr_D=2e-4,\n                 lambda_L1=100., lambda_perceptual=10.):\n        super().__init__()\n        self.device = device\n        self.lambda_L1 = lambda_L1\n        self.lambda_perceptual = lambda_perceptual\n\n        if net_G is None:\n            self.net_G = init_model(UnetGenerator(input_c=1, output_c=2))\n        else:\n            self.net_G = net_G.to(self.device)\n\n        if net_D is None:\n            self.net_D = init_model(PatchDiscriminatorSN(input_c=3))\n        else:\n            self.net_D = net_D.to(self.device)\n\n        self.GANcriterion = GANLoss(gan_mode='vanilla').to(self.device)\n        self.L1criterion = nn.L1Loss()\n        self.perceptual_loss = PerceptualLoss()\n\n        self.opt_G = optim.Adam(self.net_G.parameters(), lr=lr_G,\n                                betas=(0.5, 0.999))\n        self.opt_D = optim.Adam(self.net_D.parameters(), lr=lr_D,\n                                betas=(0.5, 0.999))\n\n    def setup_input(self, data):\n        self.L = data['L'].to(self.device)  # Input SAR image\n        self.ab = data['ab'].to(self.device)  # Ground truth ab channels\n\n    def forward(self):\n        self.fake_color = self.net_G(self.L)  # Generate fake ab channels\n\n    def backward_D(self):\n        fake_image = torch.cat([self.L, self.fake_color], dim=1)  # Concatenate L and fake ab\n        real_image = torch.cat([self.L, self.ab], dim=1)  # Concatenate L and real ab\n\n        fake_preds = self.net_D(fake_image.detach())\n        real_preds = self.net_D(real_image)\n\n        self.loss_D_fake = self.GANcriterion(fake_preds, False)\n        self.loss_D_real = self.GANcriterion(real_preds, True)\n        self.loss_D = (self.loss_D_fake + self.loss_D_real) * 0.5\n        self.loss_D.backward()\n\n    def backward_G(self):\n        fake_image = torch.cat([self.L, self.fake_color], dim=1)\n        fake_preds = self.net_D(fake_image)\n\n        self.loss_G_GAN = self.GANcriterion(fake_preds, True)\n        self.loss_G_L1 = self.L1criterion(self.fake_color, self.ab) * self.lambda_L1\n        self.loss_G_perceptual = self.perceptual_loss(self.fake_color, self.ab) * self.lambda_perceptual\n\n        self.loss_G = self.loss_G_GAN + self.loss_G_L1 + self.loss_G_perceptual\n        self.loss_G.backward()\n\n    def optimize(self):\n        # Update Discriminator\n        self.forward()\n        self.net_D.train()\n        self.opt_D.zero_grad()\n        self.backward_D()\n        self.opt_D.step()\n\n        # Update Generator\n        self.net_G.train()\n        self.opt_G.zero_grad()\n        self.backward_G()\n        self.opt_G.step()\n","metadata":{"trusted":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2025-04-07T16:11:22.370713Z","iopub.execute_input":"2025-04-07T16:11:22.370970Z","iopub.status.idle":"2025-04-07T16:11:22.387710Z","shell.execute_reply.started":"2025-04-07T16:11:22.370949Z","shell.execute_reply":"2025-04-07T16:11:22.386976Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 13: Pretraining the Generator\n\ndef pretrain_generator(net_G, train_dl, criterion, optimizer, epochs):\n    net_G.train()\n    for epoch in range(epochs):\n        loss_meter = AverageMeter()\n        for data in tqdm(train_dl):\n            L = data['L'].to(device)\n            ab = data['ab'].to(device)\n\n            preds = net_G(L)\n            loss = criterion(preds, ab)\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            loss_meter.update(loss.item(), L.size(0))\n\n        print(f\"Pretraining Epoch [{epoch+1}/{epochs}], Loss: {loss_meter.avg:.5f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T16:11:22.388665Z","iopub.execute_input":"2025-04-07T16:11:22.389002Z","iopub.status.idle":"2025-04-07T16:11:22.404769Z","shell.execute_reply.started":"2025-04-07T16:11:22.388972Z","shell.execute_reply":"2025-04-07T16:11:22.403982Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Cell A: Pretrain the Generator\n# print(\"Starting Generator Pretraining...\")\n\n# pretrain_generator(\n#     net_G=model.module.net_G,\n#     train_dl=train_dl,\n#     criterion=model.module.L1criterion,\n#     optimizer=model.module.opt_G,\n#     epochs=5  # Change as needed\n# )\n\n# print(\"Pretraining Completed.\\n\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T16:11:22.405505Z","iopub.execute_input":"2025-04-07T16:11:22.405729Z","iopub.status.idle":"2025-04-07T16:11:22.419498Z","shell.execute_reply.started":"2025-04-07T16:11:22.405711Z","shell.execute_reply":"2025-04-07T16:11:22.418731Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_model(model, train_dl, val_dl, epochs, pretrain_epochs=5, display_every=5):\n    # Pretrain Generator\n    print(\"Starting Generator Pretraining...\")\n    pretrain_generator(\n        net_G=model.module.net_G,\n        train_dl=train_dl,\n        criterion=model.module.L1criterion,\n        optimizer=model.module.opt_G,\n        epochs=pretrain_epochs\n    )\n    print(\"Pretraining Completed.\\n\")\n\n    # Training with GAN\n    for epoch in range(epochs):\n        model.module.net_G.train()\n        model.module.net_D.train()\n        loss_meter_dict = {'loss_D': AverageMeter(), 'loss_G': AverageMeter(), 'per_pixel_acc': AverageMeter()}  # Added per-pixel accuracy\n\n        for data in tqdm(train_dl):\n            model.module.setup_input(data)\n            model.module.optimize()\n\n            # Update loss meters\n            loss_meter_dict['loss_D'].update(model.module.loss_D.item(), data['L'].size(0))\n            loss_meter_dict['loss_G'].update(model.module.loss_G.item(), data['L'].size(0))\n\n        # Validation and Per-Pixel Accuracy Calculation\n        if (epoch + 1) % display_every == 0:\n            model.module.net_G.eval()\n            with torch.no_grad():\n                val_data = next(iter(val_dl))\n                L = val_data['L'].to(device)\n                ab_gt = val_data['ab'].to(device)\n\n                ab_pred = model.module.net_G(L)  # Get predictions\n\n                # Compute per-pixel accuracy\n                threshold = 0.05  # Define tolerance for color difference\n                correct_pixels = torch.abs(ab_pred - ab_gt) < threshold\n                per_pixel_accuracy = correct_pixels.float().mean().item()\n\n                loss_meter_dict['per_pixel_acc'].update(per_pixel_accuracy, L.size(0))\n\n            # Print losses and accuracy\n            print(f\"\\nEpoch [{epoch+1}/{epochs}]\")\n            print(f\"Loss_D: {loss_meter_dict['loss_D'].avg:.5f}, \"\n                  f\"Loss_G: {loss_meter_dict['loss_G'].avg:.5f}, \"\n                  f\"Per-Pixel Accuracy: {loss_meter_dict['per_pixel_acc'].avg:.5f}\")\n\n            # Save Model and Visualize\n            visualize(model.module, val_data, save=True, epoch=epoch+1)\n            torch.save(model.module.state_dict(), f'model_epoch_{epoch+1}.pth')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T16:11:22.420305Z","iopub.execute_input":"2025-04-07T16:11:22.420611Z","iopub.status.idle":"2025-04-07T16:11:22.436608Z","shell.execute_reply.started":"2025-04-07T16:11:22.420577Z","shell.execute_reply":"2025-04-07T16:11:22.435745Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Initialize the model\nmodel = MainModel()\n\n# Wrap the model with DataParallel\nmodel = nn.DataParallel(model)\n\n# Training parameters\npretrain_epochs = 5\ngan_epochs = 20\ntotal_epochs = gan_epochs\n\n# Start training\ntrain_model(\n    model=model,\n    train_dl=train_dl,\n    val_dl=val_dl,\n    epochs=total_epochs,\n    pretrain_epochs=pretrain_epochs,\n    display_every=5\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T16:11:22.437387Z","iopub.execute_input":"2025-04-07T16:11:22.437608Z","iopub.status.idle":"2025-04-07T19:43:35.029106Z","shell.execute_reply.started":"2025-04-07T16:11:22.437591Z","shell.execute_reply":"2025-04-07T19:43:35.028239Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Cell B: Train with GAN\n# def train_with_gan(model, train_dl, val_dl, epochs, display_every=5):\n#     for epoch in range(epochs):\n#         model.module.net_G.train()\n#         model.module.net_D.train()\n#         loss_meter_dict = {\n#             'loss_D': AverageMeter(), \n#             'loss_G': AverageMeter(), \n#             'per_pixel_acc': AverageMeter()\n#         }\n\n#         for data in tqdm(train_dl):\n#             model.module.setup_input(data)\n#             model.module.optimize()\n\n#             loss_meter_dict['loss_D'].update(model.module.loss_D.item(), data['L'].size(0))\n#             loss_meter_dict['loss_G'].update(model.module.loss_G.item(), data['L'].size(0))\n\n#         if (epoch + 1) % display_every == 0:\n#             model.module.net_G.eval()\n#             with torch.no_grad():\n#                 val_data = next(iter(val_dl))\n#                 L = val_data['L'].to(device)\n#                 ab_gt = val_data['ab'].to(device)\n\n#                 ab_pred = model.module.net_G(L)\n\n#                 threshold = 0.05\n#                 correct_pixels = torch.abs(ab_pred - ab_gt) < threshold\n#                 per_pixel_accuracy = correct_pixels.float().mean().item()\n\n#                 loss_meter_dict['per_pixel_acc'].update(per_pixel_accuracy, L.size(0))\n\n#             print(f\"\\nEpoch [{epoch+1}/{epochs}]\")\n#             print(f\"Loss_D: {loss_meter_dict['loss_D'].avg:.5f}, \"\n#                   f\"Loss_G: {loss_meter_dict['loss_G'].avg:.5f}, \"\n#                   f\"Per-Pixel Accuracy: {loss_meter_dict['per_pixel_acc'].avg:.5f}\")\n\n#             visualize(model.module, val_data, save=True, epoch=epoch+1)\n#             torch.save(model.module.state_dict(), f'model_epoch_{epoch+1}.pth')\n\n# # Start GAN training after pretraining\n# train_with_gan(model, train_dl, val_dl, epochs=20, display_every=5)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-07T19:43:35.030115Z","iopub.execute_input":"2025-04-07T19:43:35.030478Z","iopub.status.idle":"2025-04-07T19:43:35.034437Z","shell.execute_reply.started":"2025-04-07T19:43:35.030431Z","shell.execute_reply":"2025-04-07T19:43:35.033787Z"}},"outputs":[],"execution_count":null}]}